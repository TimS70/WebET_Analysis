---
title: "Fixation task Regression Analysis -- Offset"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---

# Setup
```{r, setup, message=FALSE, warning=FALSE}
root = "C:/Users/User/GitHub/WebET_Analysis"
path_results = file.path(root, 'results', 'plots', 'fix_task')

knitr::opts_knit$set(root.dir = normalizePath(file.path('..', '..')))

knitr::opts_knit$get("root.dir")

source(file.path(root, 'utils', 'r', 'geom_split_violin.R'))
source(file.path(root, 'utils', 'r', 'merge_by_subject.R'))
source(file.path(root, 'utils', 'r', 'summarize_datasets.R'))
source(file.path(root, 'utils', 'r', 'get_packages.R'))
source(file.path(root, 'utils', 'r', 'remove_runs.R'))
source(file.path(root, 'utils', 'r', 'add_log_k.R'))
source(file.path(root, 'utils', 'r', 'remove_na_et_indices.R'))
source(file.path(root, 'utils', 'r', 'add_x_count.R'))
source(file.path(root, 'utils', 'r', 'plot_outcome_variance.R'))

get_packages(c( 'boot',
				'boxcoxmix',
			    'broom',
			    'car',
			    'compiler',
			    'data.table',
			    'DHARMa',
			    'GGally',
			    'HLMdiag',
			    'Hmisc',
			    'influence.ME', 
			    "ICC",
			    "knitr",
			    'lme4',
			    'lattice',
			    'lme4',
			    "lmerTest", # Erhalte p-Werte
			    'nlme', 
			    'parallel',
			    'reshape',
			    'reshape2',
			    "rsq",
			    'tidyverse',
			    "tinytex"))
```


# Read and create datasets
```{r}
path = file.path(root, 'data', 'fix_task', 'added_var')

data_subject = read.csv(file.path(path, 'data_subject.csv'))
data_trial = read.csv(file.path(path, 'data_trial.csv'))
data_et = read.csv(file.path(path, 'data_et.csv'))

summarize_datasets(data_et, data_trial, data_subject)
```


## 1) Emtpy model (intercept only)
ICC = 0.46. 46% of the variance can be explained by the variance between the subjects. Interdependence assumption of the simple linear regression is violated. We should to an MLA
```{r}
lmer0_io = lmer(
    offset ~ 1 + (1 | run_id), 
    data=data_trial,
    REML=FALSE) # FML for comparing different fixed effects

summary(lmer0_io)

ICCest(factor(data_trial$run_id), data_trial$offset, alpha=.05, CI.type="THD")
```

## 2) Intermediate models

### Control variables
Control for the fact that the subjects start at different places regarding the accuracy of the eyetracking data. Standard error has increased. Subjects vary by about 0.12 (Jan 24) around the intercept. The RI model is way better than the IO model. Therefore, we should do an MLM.
```{r}
data_trial = merge_by_subject(data_trial, data_subject, 'window')

data_trial = data_trial %>%
    mutate(y_pos_c = dplyr::recode(y_pos, '0.2'=(-1L), '0.5'=0L, '0.8'=1L),
    	   x_pos_c = dplyr::recode(x_pos, '0.2'=(-1L), '0.5'=0L, '0.8'=1L),
    	   fps_c = scale(fps),
    	   window_c = scale(window))

lmer1_control = lmer(
    offset ~  withinTaskIndex + chinFirst + x_pos_c + y_pos_c + window_c + 
    	fps_c + (1 | run_id), 
    data=data_trial,
    REML=FALSE)
summary(lmer1_control)
anova(lmer0_io, lmer1_control)

lmer2_control = lmer(
    offset ~  withinTaskIndex + y_pos_c + (1 | run_id), 
    data=data_trial,
    REML=FALSE)
print('Control model: ')
summary(lmer2_control)
anova(lmer0_io, lmer2_control)
```


### Random intercept
```{r}
lmer3_experimental = lmer(
    offset ~ withinTaskIndex + y_pos_c + glasses_binary + chin + 
    	(1 | run_id), 
    data=data_trial,
    REML=FALSE)
summary(lmer3_experimental)
confint(lmer3_experimental, method="boot", n=50) # CI with Bootstrap
```



## 3) Final model: RIRS
Control variables, such as age and withinTaskIndex do not show a correlation. 
Predictors: attributeIndex + optionIndex + payneIndex
 - Effect of the Option Index: Odds for choseLL increase by the factor exp(-0.97)=0.38. The more gaze points on immediate options, the lower the odd for choseLL
 - Effect of the Payne Index: Odds for choseLL increase by the factor exp(-0.17)=0.84. The more transitions within options (positive payne), the lower the odd for chose LL
 - Did not try to get the procentual reduction of L1 and L2 variance because the logistic regression does not have error terms

```{r}
glmer_final = lmer(
    offset ~ withinTaskIndex + x_pos_c + y_pos_c + fps + chin + glasses_binary + 
    	(chin + glasses_binary | run_id), 
    data=data_trial,
    REML=FALSE)

summary(glmer_final)
# confint(glmer_final, method="boot", n=500) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
anova(lmer2_control, lmer3_experimental, glmer_final)
```


## Effects
Use formulas without random slopes (Snijders & Bosker, 2012)
```{r}
m0 <- lmer(offset ~ (1 | run_id),
           data=data_trial,
           REML=FALSE)
m1 <- lmer(offset ~ withinTaskIndex + (1 | run_id),
           data=data_trial,
           REML=FALSE)

L1_m0 <- sigma(m0)^2 # L1-Residualvarianz RIO Modell
L1_m1 <- sigma(m1)^2 # L1-Residualvarianz RI Modell
print('Pseudo R^2')
print(paste(
    'Reduction of L1 variance due to chin: ',
    (L1_m0 - L1_m1)/L1_m0
))

L2_m0 <- unlist(summary(m0)$var) # Interceptvarianz RI Modell
L2_m1 <- unlist(summary(m1)$var) # Interceptvarianz IAO Modell
print(paste(
    'Reduction of L2 variance due to chin: ',
    (L2_m0 - L2_m1)/L2_m0
))

```


## Assumptions
Sources: 
 - https://www.youtube.com/watch?v=UvyxSqEXBwc
 - https://www.pythonfordatascience.org/mixed-effects-regression-python/
 - https://ademos.people.uic.edu/Chapter18.html
 
### Linearity: Plot residuals vs. observed
```{r}
dir.create(file.path(path_results, 'assumptions'))

# Standardized residuals
data_trial$residuals = residuals(glmer_final, type='pearson')
ggplot(data_trial, aes(x=residuals, y=offset)) + 
	geom_point() + 
	ggtitle('Distribution Standardized Residuals against offset') + 
	xlab('Predicted') + 
	ylab('Standardized Residuals') +
	theme_bw()
	
ggsave(filename = file.path(path_results, 'assumptions', 'linearity.png'))
```

### Homosedasticity: Homogeneity of residuals across groups
ANOVA as a variation of Levene's test. Extract residuals from the model and square the absolute value. Should be not significant. Significant differences across residuals. We have Heteroscedasticity.
```{r}
data_trial = data_trial %>%
    mutate(res = residuals(glmer_final),
    	   res_sq = (abs(res)^2))

levene_model <- lm(res_sq ~ run_id, data=data_trial)
anova(levene_model)

jpeg(file = file.path(path_results, 'assumptions', 'heteroscedasticity.jpeg'))
plot(glmer_final)
dev.off()
```

### Normal distribution of residuals
Normal distribution not given. 
```{r}
qqmath(lmer4_rs, id=0.05)
```

Lets try with log transformed offset
Does not make it any better
```{r}
data_trial = data_trial %>%
    mutate(
        offset_log = log(offset+1),
        offset_log10 = log10(offset+1),
        offset_sqrt = sqrt(offset),
        )

lmer_log = lmer(
    offset_log ~ y_pos_c + chin + (chin | run_id), 
    data=data_trial,
    REML=FALSE)
summary(lmer_log)
# ranef(lmer_log)
qqmath(lmer_log, id=0.05, prep_global_data='log transformed offset')

lmer_log10 = lmer(
    offset_log10 ~ y_pos_c + chin + (chin | run_id), 
    data=data_trial,
    REML=FALSE)
summary(lmer_log10)
# ranef(lmer_log10)
qqmath(lmer_log10, id=0.05, prep_global_data='log10 transformed offset')

lmer_sqrt = lmer(
    offset_sqrt ~ y_pos_c + chin + (chin | run_id), 
    data=data_trial,
    REML=FALSE)
summary(lmer_sqrt)
# ranef(lmer_sqrt)
qqmath(lmer_sqrt, id=0.05, prep_global_data='sqrt transformed offset')

```

### Residuals for sqrt transformation
Does not look better
```{r}
data_trial = data_trial %>%
    mutate(
        res = residuals(lmer_sqrt),
        res_sq = (abs(res)^2))

levene_model <- lm(res_sq ~ run_id, data=data_trial)
anova(levene_model)

plot(lmer_sqrt)
```

### Multicollinearity of the predictors
No multicollinearity
```{r}
predictors = c('withinTaskIndex', 'chin', 'glasses_binary')

correlation = cor(
    data_trial %>% 
        dplyr::select(predictors),
    use = 'pairwise.complete.obs'
)

symnum(correlation)
correlation

car::vif(lmer4_rs)
```

### HLMdiag
https://cran.rstudio.com/web/packages/HLMdiag/index.html
```{r}
#case_delete() #iteratively delete groups corresponding to the levels of a hierarchical linear model, using lmer to fit the models for each deleted case

#covratio() #calculate measures of the change in the covariance matrices for the fixed effects based on the deletion of an observation, or group of observations,

#diagnostics() #is used to compute deletion diagnostics for a hierarchical linear model based on the building blocks returned by case_delete.

#HLMresid() #extracts residuals from a hierarchical linear model fit using lmer. Can provide a variety of different types of residuals based upon the specifications when you call the function

#leverage() #calculates the leverage of a hierarchical linear model fit

#mdffits() #calculate measures of the change in the fixed effects estimates based on the deletetion of an observation, or group of observations
```


# Linear model (Fixed Effects, no MLA)
Average offset=0.17 = 17%
```{r}
lm_null = lm(offset ~ 1, data = data_trial %>% filter(!is.na(offset)))
summary(lm_null)

lm_sub_0_control = lm(offset ~ window + fps + age + ethnic, 
					  data = data_subject)

summary(lm_sub_0_control)

lm_sub_1 = lm(offset ~ window + fps + age + ethnic + glasses_binary,
			  data = data_subject)
summary(lm_sub_1)
anova(lm_sub_0_control, lm_sub_1)
```

