---
title: "Fixation task Regression Analysis -- Precision"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---

# Setup
```{r}
root = "C:/Users/User/GitHub/WebET_Analysis"
path_results = file.path(root, 'results', 'plots', 'choice_task')

knitr::opts_knit$set(root.dir = normalizePath(file.path('..', '..')))

knitr::opts_knit$get("root.dir")

source(file.path(root, 'utils', 'r', 'geom_split_violin.R'))
source(file.path(root, 'utils', 'r', 'merge_mean_by_subject.R'))
source(file.path(root, 'utils', 'r', 'merge_by_subject.R'))
source(file.path(root, 'utils', 'r', 'summarize_datasets.R'))
source(file.path(root, 'utils', 'r', 'get_packages.R'))
source(file.path(root, 'utils', 'r', 'remove_runs.R'))
source(file.path(root, 'utils', 'r', 'add_log_k.R'))
source(file.path(root, 'utils', 'r', 'remove_na_et_indices.R'))
source(file.path(root, 'utils', 'r', 'add_x_count.R'))

get_packages(c('broom',
              'car', 
              'colorspace', 
              'dplyr', 
              "effsize",
              'e1071',
              'GGally',
              "ggplot2",
              "ggsignif",
              'lme4',
              'matlabr',
              'QuantPsyc',
              "RColorBrewer",
              'reshape2',
              'tidyr',
              'tidyverse'))
```


# Read and create datasets
```{r}
use_adjusted_et_data = FALSE
if (use_adjusted_et_data) {
    print('Using adjusted et_data. As a consequence there is no et data beyond the corrected aoi clusters')
}

if (use_adjusted_et_data) {
    
    data_subject = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_subject.csv'))
    
    data_trial = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_trial.csv'))
    
    data_et = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_et.csv'))

} else {

    data_subject = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_subject.csv'))
    
    data_trial = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_trial.csv'))
    
    data_et = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_et.csv'))
}

summarize_datasets(data_et, data_trial, data_subject)
```


## k-values
K values and noise
```{r}
data_subject = add_log_k(data_subject, FALSE)
data_trial = add_log_k(data_trial, FALSE)

data_subject %>%
    dplyr::select(run_id, logK, noise)

data_trial %>%
    dplyr::group_by(run_id) %>%
    dplyr::summarise(logK, noise, .groups='keep') %>%
	head(5)
```


# Screening
## Biased choices
```{r}
runs_biasedChoices = data_subject %>%
    filter(
        choseLL>0.98 | choseLL<0.02 |
        choseTop>0.98 | choseTop<0.02     
          ) %>%
    arrange(run_id) %>%
    dplyr::pull(run_id)
runs_biasedChoices
```


## Criteria distribution.
We need a sufficient ratio between the categories. Otherwise everything gets significant that happens in the larger category. 
```{r}
summary(factor(data_trial$choseLL))

summary(factor(data_trial$choseLL))[1] / 
    summary(factor(data_trial$choseLL))[2]
```


## Enough data points for the categorical predictors?
```{r}
table(data_trial$LL_top, data_trial$choseLL)
    
temp = data_trial %>% 
    merge_by_subject(data_subject, 'birthyear') %>% 
    merge_by_subject(data_subject, 'gender')

table(temp$birthyear, temp$choseLL)
table(temp$gender, temp$choseLL)

```

## Enough data points for the ET indices?
We need two more requirements to our data:
1) at least three trials with a valid option index
2) 40% of the trials have valid ET indices
```{r}
data_trial = add_x_count(data_trial, data_et)
	
grouped_n_ET = data_trial %>% 
    group_by(run_id, choseLL) %>%
    dplyr::summarise(
        n = n(),
        choice_perc = n/80,
        x_count = mean(x_count),
        n_OI = sum(!is.na(optionIndex)),
        OI = mean(optionIndex, na.rm=TRUE), 
        n_AI = sum(!is.na(attributeIndex)),
        AI = mean(attributeIndex, na.rm=TRUE),
        n_PI = sum(!is.na(payneIndex)),
        PI = mean(payneIndex, na.rm=TRUE),
        .groups='keep') %>%
    arrange(run_id, choseLL)

runs_biased_choices = grouped_n_ET %>%
    filter(n<4) %>% 
    arrange(run_id) %>%
    dplyr::pull(run_id)
print(paste('Subjects with less than 3 trials in one category: ', 
            length(runs_biased_choices)))
runs_biased_choices


min_percentage = 0

runs_na_oi = grouped_n_ET %>%
    filter(n_OI<3 | n_OI/n<min_percentage) %>%
    dplyr::pull(run_id) %>%
    unique()

print(paste('OI - length: ', 
            length(runs_na_oi))
)
runs_na_oi

runs_na_ai = grouped_n_ET %>%
    filter(n_AI<3 | n_AI/n<min_percentage) %>%
    dplyr::pull(run_id) %>%
    unique()

print(paste('AI - length: ', 
      length(runs_na_ai))
)
runs_na_ai

runs_na_pi = grouped_n_ET %>%
    filter(n_PI<3 | n_PI/n<min_percentage) %>%
    dplyr::pull(run_id) %>%
    unique()
print(paste('PI - length: ', 
      length(runs_na_pi))
)
runs_na_pi

runs_na_et = unique(
    c(runs_na_oi, runs_na_ai, runs_na_pi)
)

print(paste(
    'Subjects without enough data for ET indices within one choice option: ',
    length(runs_na_et))
)
runs_na_et

grouped_n_ET %>%
    filter(run_id %in% runs_na_et)
```

## Visualize choice variance 
```{r}
data_plot = grouped_n_ET %>%
    filter(choseLL==1) %>%
    mutate(SE = sqrt(choice_perc * (1-choice_perc)/n))

ggplot(data_plot, aes(factor(run_id), choice_perc)) + 
    geom_pointrange(aes(ymin=choice_perc-1.96*SE, 
                        ymax=choice_perc+1.96*SE)) +
    xlab('run_id') +
    ylab('Percent of choseLL') + 
    theme(
        axis.text.x = element_blank(),
        ) +
    coord_cartesian(ylim=c(0, 1))

dir.create(file.path(path_results, 'MLA'))
ggsave(file.path(path_results, 'MLA', 'runs_vs_choice.pdf'), 
	   width=5.5, height=5)
```


# Cleaning
```{r}
# Data on subject level

exclude_runs = c(
    # exclude_runs,
    runs_biasedChoices,
    runs_na_oi,
    runs_na_ai,
    runs_na_pi) %>% 
    sort() %>%
    unique()

grouped = data.frame(
    source =     c(    
        'runs_biasedChoices',
        'runs_na_oi',
        'runs_na_ai',
        'runs_na_pi', 
        'total'),
    n = c(
        length(runs_biasedChoices),
        length(runs_na_oi),
        length(runs_na_ai),
        length(runs_na_pi), 
        length(exclude_runs)
    )
)
grouped

data_subject = remove_runs(data_subject)
data_trial = remove_runs(data_trial)

data_subject = remove_na_et_indices(data_subject)
data_trial = remove_na_et_indices(data_trial)
```

# Correlative Analysis

## Scatter matrix
```{r, scatter_plot}
dir.create(file.path(path_results, 'correlations'))
ggpairs(
    data_subject %>% dplyr::select(
        choseLL,
        attributeIndex, optionIndex, payneIndex, fps),     
    progress = F) 
ggsave(file.path(path_results, 'correlations', 'corr_plot_1.pdf'), 
	   width=5.5, height=5)

ggpairs(
    data_subject %>% dplyr::select(
        choseLL, birthyear, gender, degree,
        LL_top, choice_rt, logK),
    progress = F) 
ggsave(file.path(path_results, 'correlations', 'corr_plot_2.pdf'), 
	   width=5.5, height=5)

ggpairs(
    data_trial %>% dplyr::select(
        choseLL,
        withinTaskIndex, trial_duration_exact,
        attributeIndex, optionIndex, payneIndex),     
    progress = F) 
ggsave(file.path(path_results, 'correlations', 'corr_plot_trial.pdf'),
	   width=5.5, height=5)
```

## Check withinTaskIndex and trial_duration_exact
```{r}
ggplot(data_trial, aes(x=factor(choseLL), y=withinTaskIndex)) + 
    geom_violin(trim=FALSE) +
    stat_summary(fun=mean, size=1)

ggplot(data_trial, aes(x=factor(choseLL), y=trial_duration_exact)) + 
    geom_violin(trim=FALSE) +
    stat_summary(fun=mean, size=1)
```

# Logistic Regression on subject-level
Sommet, 2017
 - choseLL is more likely than choseSS. 
 - Logisitic regression does not have residuals because it predicts probabilities not concrete values.
 - Predict log-odds (exp(beta)). Odds increase by the factor exp(beta), when the predictor changes in one unit. beta=0 means odds=1. 
 - A positive Attribute Index indicates more fixations on the amount attributes
 - A negative Option Index indicates more fixations on the delayed option
 - A negative Payne Index indicates more transitions within attributes 
 
```{r, logistic_regression, results=FALSE, echo=FALSE, message=FALSE}
m_null = glm(choseLL ~ 1,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m_null)

m1 = glm(choseLL ~ 1 + attributeIndex,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m1)
anova(m_null, m1)
```

# MLA
When I do not look at the summarized scores but on every trial, I need to account for the lack of interdependence and use a MLA.

## Intercept Only
https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
 - Odds for LL choice is exp(0.74) = 2.1 (Feb 02). Bias towards choseLL. The average chance for choseLL is exp(0.74)/(1+exp(0.74)) = 0.7. 
 - Once we control for the inter-subject variance, the choice shifts more towards SS (choseLL reduces from 65% to around 50%)
 - Mean log-odds for choice_LL vary across subjects. SD=0.4418, Odds could increase by factor exp(1.145)=3.14.
 - ICC=0.27, i.e. 27% of the variance are explained by between subject differences
```{r, MLA}
glmer0_io = glmer(
    choseLL ~ 1 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer0_io)

# take pi^2 / 3 instead of the level-1 residual
icc <- glmer0_io@theta[1]^2/ (glmer0_io@theta[1]^2 + (3.14159^2/3))
icc
```

## Random Intercept (Constrained intermediate model)
```{r, random_intercept}
data_trial$rt_c = scale(data_trial$trial_duration_exact)

glmer1_ri = glmer(
    choseLL ~ withinTaskIndex + rt_c + 
    	attributeIndex + payneIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer1_ri)
anova(glmer0_io, glmer1_ri)
```

### Random Intercept Random Slope (Augmented intermediate model)
Check if random slopes make sense.
```{r, ri_rs}
glmer2_rirs = glmer(
    choseLL ~ withinTaskIndex + rt_c + 
    	attributeIndex + payneIndex + 
    	(attributeIndex + payneIndex | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer2_rirs)
anova(glmer1_ri, glmer2_rirs)
```

### Within-Subject centering
```{r}
grouped = data.frame(
    data_subject$run_id,
    data_subject$attributeIndex,
    data_subject$optionIndex,
    data_subject$payneIndex,
    data_subject$choice_rt)

names(grouped) = c('run_id', 
                   'cluster_mean_AI', 
                   'cluster_mean_OI', 
                   'cluster_mean_PI',
				   'cluster_mean_rt')

for (col in names(grouped)[2:5]){
    if (col %in% names(data_trial)) {
        data_trial = data_trial %>% dplyr::select(!col)
    }
}

data_trial = merge(data_trial, grouped, by='run_id')

	
data_trial$attributeIndex_cmc = data_trial$attributeIndex - 
	data_trial$cluster_mean_AI

data_trial$optionIndex_cmc = data_trial$optionIndex - 
    data_trial$cluster_mean_OI

data_trial$payneIndex_cmc = data_trial$payneIndex - 
    data_trial$cluster_mean_PI

glmer3_ri = glmer(
    choseLL ~ 
        withinTaskIndex + rt_c + attributeIndex_cmc +
        payneIndex_cmc + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer3_ri)
anova(glmer0_io, glmer3_ri)

glmer4_rirs = glmer(
    choseLL ~  withinTaskIndex + rt_c + attributeIndex_cmc +
        payneIndex_cmc + (attributeIndex_cmc + payneIndex_cmc | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer4_rirs)
anova(glmer3_ri, glmer4_rirs)
```

## Fial model: RIRS
Control variables, such as age and withinTaskIndex do not show a correlation. 
Predictors: attributeIndex + optionIndex + payneIndex
 - Effect of the Option Index: Odds for choseLL increase by the factor exp(-0.97)=0.38. The more gaze points on immediate options, the lower the odd for choseLL
 - Effect of the Payne Index: Odds for choseLL increase by the factor exp(-0.17)=0.84. The more transitions within options (positive payne), the lower the odd for chose LL
 - Did not try to get the procentual reduction of L1 and L2 variance because the logistic regression does not have error terms

```{r}
data_trial$rt_c = scale(data_trial$trial_duration_exact)

glmer_final = glmer(
    choseLL ~ withinTaskIndex + rt_c + 
    	attributeIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

summary(glmer_final)
# confint(glmer_final, method="boot", n=50) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
```


## Simple Slopes
```{r}
# SD
sd_ai = sd(data_trial$attributeIndex, na.rm = TRUE)

# Simple Slope 1
data_trial$ai_sd_1 = data_trial$attributeIndex + sd_ai
# Simple Slope -1
data_trial$ai_sd_2 = data_trial$attributeIndex - sd_ai

glmer_final_sd_1 = glmer(
    choseLL ~ withinTaskIndex + rt_c + ai_sd_1 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd_1)

glmer_final_sd_2 = glmer(
    choseLL ~ withinTaskIndex + rt_c + ai_sd_2 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd_2)
```

## Odds Ratios 
 - The effect of "payneIndex_cmc" is not significant, payneIndex_cmc OR = 0.83, 95% CI [0.67, 1] (has to be beyond 1 to be significant). Imagine it to be signficant: with an increase of the Payne Index of 1 (e.g. if they switch from neutral to only within option transitions), results in 0.67 times the chance (a lower chance) of choosing the delayed reward. 
 - Same for the ones with low (-1 SD) and high (+1 SD) Payne Index (If you have random slope, compare the different levels. 

```{r}
OR <- exp(fixef(glmer_final))
CI <- exp(confint(glmer_final, parm="beta_")) # it can be slow (~ a few minutes). As alternative, the much faster but less precise Wald's method can be used: CI <- exp(confint(FM,parm="beta_",method="Wald")) 

or_sd_1 <- exp(fixef(glmer_final_sd_1))
ci_sd_1 <- exp(confint(glmer_final_sd_1, parm="beta_")) 

or_sd_2 <- exp(fixef(glmer_final_sd_2))
ci_sd_2 <- exp(confint(glmer_final_sd_2, parm="beta_")) 

OR_CI<-rbind(
    cbind(OR,CI), 
    cbind(or_sd_1, ci_sd_1)[3,], 
    cbind(or_sd_2, ci_sd_2)[3,])

rownames(OR_CI) = c(
    rownames(cbind(OR,CI)), 
    "payne_cmc_sd1", 
    "payne_cmc_sd0")
OR_CI
```

# Clusters
No clusters can improve the prediction of choices
```{r}
data_trial$rt_c = scale(data_trial$trial_duration_exact)

m_cluster_1 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_2 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster2 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_3 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster3 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_4 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster4 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

fixef(m_cluster_2)[5]
fixef(m_cluster_3)[5]
fixef(m_cluster_4)[5]

# confint(m_cluster_2, method="boot", n=50) # Bootstrap
anova(m_cluster_1, m_cluster_2, m_cluster_3, m_cluster_4)
```

# Assumptions

## Linear relationship between predicted log(choseLL) and the predictors
Looks quite linear. If not check out: http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/
```{r}
predictors = c('attributeIndex', 'optionIndex', 'payneIndex')

data_plot = data_trial %>%
    mutate(predict_choseLL_prob = 
        predict(glmer_final, type = "response"),
        logit = log(predict_choseLL_prob/(1-predict_choseLL_prob))) %>%
    dplyr::select(c(logit, predictors)) %>%
    gather(key = "predictors", value = "predictor_value", -logit)

ggplot(data_plot, aes(logit, predictor_value)) +
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") + 
    theme_bw() + 
    facet_wrap(~predictors, scales = "free_y")
dir.create(file.path(path_results, 'MLA', 'assumptions'))
ggsave(file.path(path_results, 'MLA', 'assumptions', 'linearity.pdf'), 
	   width=5.5, height=5)
```

## Multicollinearity
compute variance inflation factor. " As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity"
```{r}
car::vif(glmer_final)
```

## DHARMa
"DHARMa was created by Florian Hartig in 2016 and creates readily interpretable residuals for generalized linear (mixed)"
https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
```{r}
#simulateResiduals() #creates scaled (quantile) residuals through a default 250 simulations (which can be modified)

#plotSimulatedResiduals #provides qqplot and residuals vs predicted plots to determine deviations from normality

#Goodness of fit tests:
#testUniformity()
#testOverdispersion()
#testZeroinflation()
#testTemporalAutocorrelation()
#testSpatialAutocorrelation()
```