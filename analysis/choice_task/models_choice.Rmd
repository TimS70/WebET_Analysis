---
title: "Fixation task Regression Analysis -- Precision"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---

# Setup
```{r, setup, message=FALSE, warning=FALSE}
root = "C:/Users/User/GitHub/WebET_Analysis"
path_results = file.path(root, 'results', 'plots', 'choice_task')

knitr::opts_knit$set(root.dir = normalizePath(file.path('..', '..')))

knitr::opts_knit$get("root.dir")

source(file.path(root, 'utils', 'r', 'geom_split_violin.R'))
source(file.path(root, 'utils', 'r', 'merge_mean_by_subject.R'))
source(file.path(root, 'utils', 'r', 'merge_by_subject.R'))
source(file.path(root, 'utils', 'r', 'summarize_datasets.R'))
source(file.path(root, 'utils', 'r', 'get_packages.R'))
source(file.path(root, 'utils', 'r', 'remove_runs.R'))
source(file.path(root, 'utils', 'r', 'add_log_k.R'))
source(file.path(root, 'utils', 'r', 'remove_na_et_indices.R'))
source(file.path(root, 'utils', 'r', 'add_x_count.R'))
source(file.path(root, 'utils', 'r', 'identify_bad_choice_runs.R'))

get_packages(c('broom',
              'car', 
              'colorspace', 
              'dplyr', 
              "effsize",
              'e1071',
              'GGally',
              "ggplot2",
              "ggsignif",
			  'Hmisc',
              'lme4',
              'matlabr',
              'QuantPsyc',
              "RColorBrewer",
              'reshape2',
              'tidyr',
              'tidyverse'))
```


# Read and create datasets
```{r, read_data}
use_adjusted_et_data = FALSE
if (use_adjusted_et_data) {
    print('Using adjusted et_data. As a consequence there is no et data beyond the corrected aoi clusters')
}

if (use_adjusted_et_data) {
    
    data_subject = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_subject.csv'))
    
    data_trial = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_trial.csv'))
    
    data_et = read.csv(
        file.path(root, 'data', 'choice_task', 'adjusted', 
                  'data_et.csv'))

} else {

    data_subject = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_subject.csv'))
    
    data_trial = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_trial.csv'))
    
    data_et = read.csv(
        file.path(root, 'data', 'choice_task', 'uncorrected', 
                  'data_et.csv'))
}

summarize_datasets(data_et, data_trial, data_subject)
```


# Visualize choice variance 
```{r, choice_var_plot}
grouped = data_trial %>%
	group_by(run_id, choseLL) %>%
	dplyr::summarise(
		n = n(),
		.groups = 'keep') %>%
	mutate(choice_perc = n / 80)

data_plot = grouped %>%
    filter(choseLL==1) %>%
    mutate(SE = sqrt(choice_perc * (1-choice_perc)/n))

ggplot(data_plot, aes(factor(run_id), choice_perc)) + 
    geom_pointrange(aes(ymin=choice_perc-1.96*SE, 
                        ymax=choice_perc+1.96*SE)) +
    xlab('run_id') +
    ylab('Percent of choseLL') + 
    theme(
        axis.text.x = element_blank(),
        ) +
    coord_cartesian(ylim=c(0, 1))

dir.create(file.path(path_results, 'MLA'), 
		   showWarnings = FALSE)
ggsave(file.path(path_results, 'MLA', 'runs_vs_choice.pdf'), 
	   width=5.5, height=5)
```

# Check distribution over categorical variables
Enough data points for the categorical predictors? We need a sufficient ratio between the categories. Otherwise everything gets significant that happens in the larger category. 
```{r, freqency_tables}
cat(paste('Mean choseLL:', mean(data_trial$choseLL), '\n'))

cat('\nLL_top vs. choseLL')
table(data_trial$LL_top, data_trial$choseLL)
    
temp = data_trial %>% 
    merge_by_subject(data_subject, 'birthyear') %>% 
    merge_by_subject(data_subject, 'gender')

cat('\nbirthyear vs. choseLL')
temp$birthyear_bin = cut(
                temp$birthyear, 
                breaks = c(0, 1950, 1960, 1970, 1980, 1990, 2000, 2010),
                labels = c(1950, 1960, 1970, 1980, 1990, 2000, 2010),
                include.lowest=TRUE)
table(temp$birthyear_bin, temp$choseLL)

cat('\ngender vs. choseLL')
table(temp$gender, temp$choseLL)

```

# Correlative Analysis

## Scatter matrix
```{r, corr_matrix, warning=FALSE}
dir.create(file.path(path_results, 'correlations'),  
		   showWarnings = FALSE)

sapply(
	data_subject %>%
		dplyr::select(
			'choseLL', 'attributeIndex', 'optionIndex', 'payneIndex', 
	    	'fps', 'birthyear', 'LL_top', 'choice_rt', 'logK'),
	function(x) sum(!is.na(x))
)

ggpairs(
    data_subject,
    columns = c('choseLL', 'attributeIndex', 'optionIndex', 'payneIndex', 
    			'fps', 'birthyear', 'LL_top', 'choice_rt', 'logK'), 
    upper = list(continuous = wrap("cor", size = 2)),
    lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)),
    progress = F) + 
	theme_bw(base_size = 8)
ggsave(file.path(path_results, 'correlations', 'corr_plot_subject.pdf'), 
	   width=5.5, height=5)


ggpairs(
    data_trial,
    columns = c('choseLL', 'withinTaskIndex', 'trial_duration_exact',
    			'fps',
    			'attributeIndex', 'optionIndex', 'payneIndex'), 
    upper = list(continuous = wrap("cor", size = 4)),
    lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1)),
    progress = F) +
	theme_bw()
ggsave(file.path(path_results, 'correlations', 'corr_plot_trial.pdf'),
	   width=5.5, height=5)
```

## ANOVA
```{r, anova}
summary(aov(data_subject$choseLL ~ data_subject$gender))
summary(aov(data_subject$choseLL ~ data_subject$degree))
```


# Logistic Regression on subject-level
 - See Sommet, 2017
 - choseLL is more likely than choseSS. 
 - Logisitic regression does not have residuals because it predicts probabilities not concrete values.
 - Predict log-odds (exp(beta)). Odds increase by the factor exp(beta), when the predictor changes in one unit. beta=0 means odds=1. 
 - A positive Attribute Index indicates more fixations on the amount attributes
 - A negative Option Index indicates more fixations on the delayed option
 - A negative Payne Index indicates more transitions within attributes 
 
```{r, glm}
m_null = glm(choseLL ~ 1,
         data = data_subject %>%
		 	filter(!is.na(attributeIndex) & !is.na(payneIndex)), 
         family = binomial(link = "logit"))
summary(m_null)

m1 = glm(choseLL ~ 1 + choice_rt + attributeIndex + payneIndex,
         data = data_subject %>%
		 	filter(!is.na(attributeIndex) & !is.na(payneIndex)), 
         family = binomial(link = "logit"))
summary(m1)
anova(m_null, m1)
```

# MLA
When I do not look at the summarized scores but on every trial, I need to account for the lack of interdependence and use a MLA.

## Step 1: Empty model (Intercept Only)
https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
```{r, glmer0_io}
glmer0_io = glmer(
    choseLL ~ 1 + (1 | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex) & !is.na(payneIndex)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer0_io)

# take pi^2 / 3 instead of the level-1 residual
icc <- glmer0_io@theta[1]^2/ (glmer0_io@theta[1]^2 + (3.14159^2/3))

print(paste(
	'ICC=', round(icc, 2), ': ',
	round(icc, 2)*100, 
	'%  of the variance are explained by between subject differences'))

rand_std = as.data.frame(VarCorr(glmer0_io))$sdcor
print(paste(
	'Mean log-odds for choice_LL vary across subjects. SD=',
	round(rand_std, 2), '.', 
	'Odds could increase by factor exp(rand_std)=',
	round(exp(rand_std), 2)
))

beta_0 = fixef(glmer0_io)[1][1]
prob_intercept = exp(beta_0)/(1+exp(beta_0))

print(paste(
	'Bias towards choseLL: Odds for LL choice is ', 
	'exp(beta_0) = ',
	round(exp(beta_0), 2), 
	'The average probability for choseLL is exp(beta_0)/(1+exp(beta_0)) = ',
	round(prob_intercept, 2)))
```

## Step 2: Intermediate model

### Random Intercept
No significant effect but better model
```{r, random_intercept}
data_trial$rt_c = scale(data_trial$trial_duration_exact)

glmer1_ri = glmer(
    choseLL ~ withinTaskIndex + rt_c + 
    	attributeIndex + payneIndex + (1 | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex) & !is.na(payneIndex)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer1_ri)
anova(glmer0_io, glmer1_ri)

beta_0 = fixef(glmer1_ri)[1][1]
beta_0
coef_rt = fixef(glmer1_ri)[3]

effect_rt = exp(beta_0 + coef_rt) / (1+(exp(beta_0+coef_rt)))
cat(paste('Choice for LL decrease by factor', 
		  round(effect_rt, 2), 
		  'as rt increases by 1 SD in rt.'))
```


### Random Slope (Augmented intermediate model)
Random effects model does not fit better.
```{r, ri_rs}
glmer2_rirs = glmer(
    choseLL ~ withinTaskIndex + rt_c + 
    	attributeIndex + payneIndex  + 
    	(attributeIndex + payneIndex  | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex) & !is.na(payneIndex)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer2_rirs)
anova(glmer1_ri, glmer2_rirs)
```


### Within-Subject centering
No effects for within-subject centered variables
```{r, centering}
grouped = data.frame(
    data_subject$run_id,
    data_subject$attributeIndex,
    data_subject$optionIndex,
    data_subject$payneIndex,
    data_subject$choice_rt)

names(grouped) = c('run_id', 
                   'cluster_mean_AI', 
                   'cluster_mean_OI', 
                   'cluster_mean_PI',
				   'cluster_mean_rt')

for (col in names(grouped)[2:5]){
    if (col %in% names(data_trial)) {
        data_trial = data_trial %>% dplyr::select(!col)
    }
}

data_trial = merge(data_trial, grouped, by='run_id')

	
data_trial$attributeIndex_cmc = data_trial$attributeIndex - 
	data_trial$cluster_mean_AI

data_trial$optionIndex_cmc = data_trial$optionIndex - 
    data_trial$cluster_mean_OI

data_trial$payneIndex_cmc = data_trial$payneIndex - 
    data_trial$cluster_mean_PI

glmer0_io = glmer(
    choseLL ~ 1 + withinTaskIndex + rt_c + (1 | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex_cmc) & !is.na(payneIndex_cmc)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

glmer3_ri = glmer(
    choseLL ~ 
        withinTaskIndex + rt_c + attributeIndex_cmc + payneIndex_cmc + 
    	(1 | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex_cmc) & !is.na(payneIndex_cmc)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer3_ri)

glmer4_rirs = glmer(
    choseLL ~  withinTaskIndex + rt_c + attributeIndex_cmc + payneIndex_cmc + 
    	(attributeIndex_cmc + payneIndex_cmc | run_id), 
    data = data_trial %>%
		 	filter(!is.na(attributeIndex_cmc) & !is.na(payneIndex_cmc)), 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer4_rirs)
anova(glmer0_io, glmer3_ri, glmer4_rirs)
```

## Fial model: RIRS
 - No effects but better model fit
 - No Pseudo R² because it is a logistic ME regression model
```{r, glmer_final}
glmer_final = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + payneIndex + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

summary(glmer_final)
confint(glmer_final, method="boot", n=100) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
```


## Simple Slopes
Should you include Payne Index? 
```{r, simple_slopes}
# SD
sd_ai = sd(data_trial$attributeIndex, na.rm = TRUE)

# Simple Slope 1
data_trial$ai_sd_1 = data_trial$attributeIndex + sd_ai
# Simple Slope -1
data_trial$ai_sd_2 = data_trial$attributeIndex - sd_ai

glmer_final_sd_1 = glmer(
    choseLL ~ withinTaskIndex + rt_c + ai_sd_1 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd_1)

glmer_final_sd_2 = glmer(
    choseLL ~ withinTaskIndex + rt_c + ai_sd_2 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd_2)
```


## Odds Ratios 
 - The effect of "payneIndex_cmc" is not significant, payneIndex_cmc OR = 0.83, 95% CI [0.67, 1] (has to be beyond 1 to be significant). Imagine it to be signficant: with an increase of the Payne Index of 1 (e.g. if they switch from neutral to only within option transitions), results in 0.67 times the chance (a lower chance) of choosing the delayed reward. 
 - Same for the ones with low (-1 SD) and high (+1 SD) Payne Index (If you have random slope, compare the different levels. 

```{r, effects}
OR <- exp(fixef(glmer_final))
CI <- exp(confint(glmer_final, parm="beta_")) # it can be slow (~ a few minutes). As alternative, the much faster but less precise Wald's method can be used: CI <- exp(confint(FM,parm="beta_",method="Wald")) 

or_sd_1 <- exp(fixef(glmer_final_sd_1))
ci_sd_1 <- exp(confint(glmer_final_sd_1, parm="beta_")) 

or_sd_2 <- exp(fixef(glmer_final_sd_2))
ci_sd_2 <- exp(confint(glmer_final_sd_2, parm="beta_")) 

OR_CI<-rbind(
    cbind(OR,CI), 
    cbind(or_sd_1, ci_sd_1)[3,], 
    cbind(or_sd_2, ci_sd_2)[3,])

rownames(OR_CI) = c(
    rownames(cbind(OR,CI)), 
    "attributeIndex_cmc_sd1", 
    "attributeIndex_cmc_sd2")
OR_CI
```

# Clusters
No clusters can improve the prediction of choices
```{r, clusters}
data_trial$rt_c = scale(data_trial$trial_duration_exact)

m_cluster_1 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_2 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster2 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_3 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster3 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

m_cluster_4 = glmer(
    choseLL ~ withinTaskIndex + rt_c + attributeIndex + cluster4 + 
    	(1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)

fixef(m_cluster_2)[5]
fixef(m_cluster_3)[5]
fixef(m_cluster_4)[5]

# confint(m_cluster_2, method="boot", n=50) # Bootstrap
anova(m_cluster_1, m_cluster_2, m_cluster_3, m_cluster_4)
```

# Assumptions

## Linear relationship between predicted log(choseLL) and the predictors
Looks quite linear. If not check out: http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/
```{r, linearity}
predictors = c('attributeIndex', 'optionIndex', 'payneIndex')

data_plot = data_trial %>%
	filter(
		!is.na(attributeIndex) & 
		!is.na(optionIndex) & 
		!is.na(payneIndex)) %>%
    mutate(predict_choseLL_prob = 
        predict(glmer_final, type = "response"),
        logit = log(predict_choseLL_prob/(1-predict_choseLL_prob))) %>%
    dplyr::select(c(logit, predictors)) %>%
    gather(key = "predictors", value = "predictor_value", -logit)

ggplot(data_plot, aes(logit, predictor_value)) +
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") + 
    theme_bw() + 
    facet_wrap(~predictors, scales = "free_y")
dir.create(file.path(path_results, 'MLA', 'assumptions'),  
		   showWarnings = FALSE)
ggsave(file.path(path_results, 'MLA', 'assumptions', 'linearity.pdf'), 
	   width=5.5, height=5)
```

## Multicollinearity
compute variance inflation factor. " As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity"
```{r, multicollinearity}
car::vif(glmer_final)
```

## DHARMa
"DHARMa was created by Florian Hartig in 2016 and creates readily interpretable residuals for generalized linear (mixed)"
https://cran.r-project.org/web/packages/DHARMa/vignettes/DHARMa.html
```{r}
#simulateResiduals() #creates scaled (quantile) residuals through a default 250 simulations (which can be modified)

#plotSimulatedResiduals #provides qqplot and residuals vs predicted plots to determine deviations from normality

#Goodness of fit tests:
#testUniformity()
#testOverdispersion()
#testZeroinflation()
#testTemporalAutocorrelation()
#testSpatialAutocorrelation()
```