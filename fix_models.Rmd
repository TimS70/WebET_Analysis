---
title: "MixedRegression"
author: "Tim Schneegans"
date: "4 Januar 2021"
output: html_document
---

```{r setup, message=FALSE, warning=FALSE}
setwd("C:/Users/User/GitHub/WebET_Analysis")
getPackages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

getPackages(c(
    'boot',
    'broom',
    'car',
    'compiler',
    'data.table',
    'DHARMa',
    "dplyr",
    'GGally',
    'ggplot2',
    'HLMdiag',
    'Hmisc',
    'influence.ME', 
    "ICC",
    "knitr",
    'lme4',
    'lattice',
    'lme4',
    "lmerTest", # Erhalte p-Werte
    'nlme', 
    'parallel',
    'readr',
    'reshape',
    'reshape2',
    "rsq",
    "tinytex"))

knitr::opts_chunk$set(echo = TRUE)
options(scipen=999) # Show R markdown output as Integers
```

# Prep data
```{r data}
data_et <- read_csv('data_jupyter/data_et_fix.csv')
data_trial <- read_csv('data_jupyter/data_trial_fix.csv')
data_subject <- read_csv('data_jupyter/data_subject.csv')
```

## Add glasses to data_et
```{r}
merge_by_subject = function(data, data_source, varName) {
    if (varName %in% names(data)) {
        data = data %>% dplyr::select(!varName)
    }

    data = merge(
        data, 
        data_source %>% dplyr::select(run_id, varName), 
        by='run_id')
    return(data)
}

data_trial = data_trial %>%
    merge_by_subject(data_subject, 'glasses_binary')

```

# Screening
```{r}
summary(data_et %>%
    dplyr::select(x, y, t_task)
)

summary(data_trial %>% 
    dplyr::select(run_id, offset, precision, fps)
)
```

## FPS for fixation task
```{r}
trialFPS = data_et %>%
    group_by(run_id, trial_index) %>%
    summarise(n = n())

trialFPS = merge(
    trialFPS, 
    data_trial %>% 
        dplyr::select(run_id, trial_index, trial_duration_exact),
    by=c('run_id', 'trial_index'))
trialFPS$fps = 1000 * trialFPS$n / trialFPS$trial_duration_exact

if ('fps_fix' %in% names(data_subject))
    {data_subject = data_subject %>% select(!fps_fix)}
data_subject = merge(
    data_subject, 
    trialFPS %>% group_by(run_id) %>% summarise(fps_fix = mean(fps)),
    by='run_id'
)
summary(data_subject$fps_fix)
```

## Low FPS or not following instructions
```{r}
data_subject %>%
    dplyr::select(run_id, triedChin, keptHead, fps_fix) %>%
    filter(
        triedChin==0 | 
        keptHead==0 |
        fps_fix<3)

subjects_lowFPS = data_subject %>%
    dplyr::select(run_id, triedChin, keptHead, fps_fix) %>%
    filter(fps_fix<3) %>%
    dplyr::pull(run_id)
print(paste(
    'N Subjects with low FPS: ', 
    length(subjects_lowFPS)))
print(subjects_lowFPS)

subjects_noInstruction = data_subject %>%
    dplyr::select(run_id, triedChin, keptHead, fps_fix) %>%
    filter(triedChin==0 | keptHead==0) %>%
    dplyr::pull(run_id)
print(paste(
    'N Subjects who did not follow the instructions: ', 
    length(subjects_noInstruction)))
print(subjects_noInstruction)
```

## Trials with insanely high t_task
```{r}
high_t_task = data_et %>%
    filter(t_task>5500) %>%
    group_by(run_id, trial_index) %>%
    summarise(t_task_max = max(t_task))

merge_by_index = function(data, data_source, varName) {
    if ('trial_duration_exact' %in% names(high_t_task)) {
        data = data %>% dplyr::select(!varName)
    }
    data = merge(
        data, 
        data_source %>%
            dplyr::select(run_id, trial_index, varName),
        by=c('run_id', 'trial_index'))
    return(data)
}

high_t_task = high_t_task %>%
    merge_by_index(data_trial, 'trial_duration_exact')
unique(high_t_task$run_id)
high_t_task      
```

## Anyone without the full number of trials?
```{r}
# Number of trials per subject
ans = as.data.table(
    data_trial %>% 
        dplyr::select(run_id, trial_index, chin, withinTaskIndex)
    )[, count := uniqueN(trial_index), by = run_id]
ans %>% 
    filter(count!=18)
```

## Glasses 
```{r}
data_subject %>%
    dplyr::select(run_id, glasses_binary) %>%
    filter(is.na(glasses_binary))

subjects_NA_glasses = data_subject %>%
    dplyr::select(run_id, glasses_binary) %>%
    filter(is.na(glasses_binary)) %>%
    dplyr::pull(run_id)
```

# Cleaning
## Subjects to exclude from screening
```{r}
excludedSubjects = c(
    1, 70, 80, 97, 131, 2012, 
    subjects_lowFPS,
    subjects_noInstruction,
    subjects_NA_glasses) %>% 
    sort() %>%
    unique()

print(paste(
    'N Excluded subjects: ', 
    length(excludedSubjects)))
print(excludedSubjects)
```

## data_trial
```{r}
generalCleaning = function(data) {
    data_name = deparse(substitute(data))
    print(data_name)
    print(paste('Raw: ', nrow(data)))
    data = data %>%
        filter(!(run_id %in% excludedSubjects))
    print(paste('Cleaned: ', nrow(data)))
    return(data)
}

data_trial = generalCleaning(data_trial)
data_subject = generalCleaning(data_subject)
```


```{r}
print(paste('Raw: ', nrow(data_trial)))

data_trial = data_trial %>%
    filter(
        !is.na(precision) & 
        (trial_duration_exact<5100)
        )

print(paste('Cleaned: ', nrow(data_trial)))
```

## data_et
On purpose, we don't exclude specific x and y values, neither positive nor negative outliers
```{r}
print(paste('Raw: ', nrow(data_et)))
data_et = data_et %>% filter((t_task<5100))
print(paste('Cleaned: ', nrow(data_et)))
```

# Assumptions - Long Format
```{r}
predictors = c('withinTaskIndex', 'x_pos', 'y_pos', 'fps', 'glasses_binary', 'chin')
```

## Multicollinearity of the predictors
```{r}
correlation = cor(
    data_trial %>% 
        select(predictors),
    use = 'pairwise.complete.obs'
)

symnum(correlation)
correlation
```

## Standardized residuals
```{r}
random = rchisq(nrow(data_trial), 7)
fake = lm(
    random ~ ., 
    data=data_trial %>% select(predictors)
    )

fitted = scale(fake$fitted.values)
# Studentized residuals
standardized = rstudent(fake)
qqnorm(standardized)
abline(0, 1)
```

```{r}
hist(standardized)
```

## Heteroscedasticity
```{r}
plot(fitted, standardized)
abline(0, 0)
abline(v=0)
```


# Visualize Predictors
```{r}
ggpairs(data_trial[, append(predictors, c('precision', 'offset'))],
        progress=F) # Correlations
```



# Models

# Linear model (Fixed Effects, no MLA)
```{r}
lm_null = lm(offset ~ 1, data = data_trial %>% filter(!is.na(offset)))
summary(lm_null)
```

## IO: intercept only
ICC = 0.6182. 62% of the variance can be explained by the variance between the subjects. We should to an MLA
```{r}
lme0_io = lme(
    offset ~ 1, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~1 | run_id
    )

summary(lme0_io)

ICCest(factor(data_trial$run_id), data_trial$offset, alpha=.05, CI.type="THD")
```

## RI: Random intercept
Control for the fact that the subjects start at different places regarding the accuracy of the eyetracking data. Standard error has increased. Subjects vary by about 0.12 (Jan 24) around the intercept. The RI model is way better than the IO model. Therefore, we should do an MLM.

```{r}
data_trial = data_trial %>%
    mutate(
        y_pos_c = scale(y_pos),
        x_pos_c = scale(x_pos),
        fps_c = scale(fps))

lme1_control = lme(
    offset ~ x_pos_c + y_pos_c + withinTaskIndex + fps_c, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~1 | run_id
    )

summary(lme1_control)
anova(lme0_io, lme1_control)
```

Only relevant control variables
```{r}
lme2_control = lme(
    offset ~ y_pos_c, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~1 | run_id
    )

summary(lme2_control)
anova(lme0_io, lme2_control)
```


# m_ri_1: adding chin rest
Look at the predictor correlation
```{r}
lme3_experimental = lme(
    offset ~ y_pos_c + glasses_binary + chin, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~1 | run_id
    )

summary(lme3_experimental)
anova(lme0_io, lme3_experimental)
```

Only relevant variables
```{r}
lme4_experimental = lme(
    offset ~ y_pos_c + chin, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~1 | run_id
    )

summary(lme4_experimental)
```

# m_rs_0: Random slopes for chin
Standard deviation across glasses is large. Participants vary a lot around the average score. Probably another variable. Also interpret the Correlation between the fixed effects. With lower offset, chin-rest could have a greater effect. However, there are no significant findings. (Unfortunately), the RS-model is better, so the effect of chin-rest largely varies.

```{r}
lme5_rs = lme(
    offset ~ y_pos_c + chin, 
    data=data_trial,
    method='ML', 
    na.action='na.omit', 
    random = ~ chin | run_id,
    control = lmeControl(msMaxIter = 200) # Get sure it converges
    )
summary(lme5_rs)
anova(lme4_experimental, lme5_rs)
```

# Only on top positions
# Also try only using the top positions (see Semmelmann & Weigelt) 
```{r}
lme_yPos_top = lme(
    offset ~ chin + glasses_binary, 
    data=data_trial %>% filter(y_pos==0.2),
    method='ML', 
    na.action='na.omit', 
    random = ~ chin | run_id,
    control = lmeControl(msMaxIter = 200) # Get sure it converges
    )
summary(lme_yPos_top)
```

# Assumptions
For this model, 
```{r}
screen = lme(offset ~ chin, 
             data = data_trial, 
             method = 'ML',
             na.action = "na.omit",
             random = ~ chin | run_id,
             control = lmeControl(msMaxIter = 200))
standardized = as.data.frame(scale(screen$residuals))
standardized = standardized$fixed
fitted = scale(fitted.values(screen))
qqnorm(standardized)
abline(0, 1)
```

Normality got worse
```{r}
hist(standardized)
```
Heteroscedasticity got worse
Concentration of dots is heavily on the left side
```{r}
plot(fitted, standardized)
abline(0, 0)
abline(v=0)
```


## Effects
Varianzaufkl?rung: Pseudo-R? (VL11 F18f.)

# More models? 
Kontexteffekte (GRO-Zentrierung, VL10)?

# Assumptions
Sources: 
 - https://www.youtube.com/watch?v=UvyxSqEXBwc
 - https://www.pythonfordatascience.org/mixed-effects-regression-python/

## Accuracy
```{r}
```

## Outliers
```{r}
```

## Multicollinearity
```{r}
car::vif(glmer_final)
```

## Linearity
```{r}


predictors <- c(
    'attributeIndex_cmc', 'optionIndex_cmc', 
    'payneIndex_cmc')

data_plot = data_trial %>%
    mutate(predict_choseLL_prob = 
        predict(glmer_final, type = "response"),
        logit = log(predict_choseLL_prob/(1-predict_choseLL_prob))) %>%
    dplyr::select(c(logit, predictors)) %>%
    gather(key = "predictors", value = "predictor_value", -logit)
data_plot

ggplot(data_plot, aes(logit, predictor_value))+
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") + 
    theme_bw() + 
    facet_wrap(~predictors, scales = "free_y")
```
```

## Normality
Voraussetzungen (Eid2015, p. 747)
 - Residuen unabh?ngig und identisch normalverteilt, u.a.
```{r}
```

## Homogeneity
```{r}
```

## Homoscedasticity
```{r}
```