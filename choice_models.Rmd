---
title: "Choice Task Regression Analysis"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---


```{r}
setwd("C:/Users/User/GitHub/WebET_Analysis")
getPackages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

getPackages(c('broom',
              'car', 
              'colorspace', 
              'dplyr', 
              "effsize",
              'e1071',
              'GGally',
              "ggplot2",
              "ggsignif",
              'lme4',
              'matlabr',
              'QuantPsyc',
              "RColorBrewer",
              'reshape2',
              'tidyr',
              'tidyverse')
            )
```

# Read and create datasets
```{r}
data_subject <- read.table('data_jupyter/data_subject.csv', 
                           header=TRUE, sep=',')
data_subject

data_trial <- read.table('data_jupyter/data_trial_choice.csv',
                                header=TRUE, sep=',')
data_trial
```

## k-values
K values and noise
```{r}
merge_by_subject = function(data, data_source, varName) {
    if (varName %in% names(data)) {
        data = data %>% dplyr::select(!varName)
    }

    data = merge(
        data, 
        data_source %>% dplyr::select(run_id, varName), 
        by='run_id')
    return(data)
}

# run_matlab_script('fitK/fit_discount_k.m')
logK <- read.table('fitK/logK.csv', header=TRUE, sep=',')
data_subject = merge_by_subject(data_subject, logK, 'logK')
data_subject = merge_by_subject(data_subject, logK, 'noise')

data_trial = merge_by_subject(data_trial, logK, 'noise')
data_trial = merge_by_subject(data_trial, logK, 'logK')
data_subject
data_subject %>%
    dplyr::select(run_id, logK, noise)
```

# Screening

## Criteria distribution. We need a sufficient ratio between the categories. Otherwise everything gets significant that happens in the larger category. 
```{r}
summary(factor(data_trial$choseLL))

summary(factor(data_trial$choseLL))[1] / 
    summary(factor(data_trial$choseLL))[2]
```

## Enough data points for the categorical predictors?
```{r}
table(data_trial$LL_top, data_trial$choseLL)
    
temp = data_trial %>% 
    merge_by_subject(data_subject, 'birthyear') %>% 
    merge_by_subject(data_subject, 'gender')

table(temp$birthyear, temp$choseLL)
table(temp$gender, temp$choseLL)
```

## Enough data points for the ET indices?
We need two more requirements to our data:
1) at least three trials with a valid option index
2) 40% of the trials have valid ET indices
```{r}
grouped_n_ET = data_trial %>% 
    group_by(run_id, choseLL) %>%
    summarise(
        n_choseLL = n(),
        choseLL_perc = n_choseLL/80,
        x_count = mean(x_count),
        n_OI = sum(!is.na(optionIndex)),
        OI = mean(optionIndex, na.rm=TRUE), 
        n_AI = sum(!is.na(attributeIndex)),
        AI = mean(attributeIndex, na.rm=TRUE),
        n_PI = sum(!is.na(payneIndex)),
        PI = mean(payneIndex, na.rm=TRUE)
        ) %>%
    arrange(run_id, choseLL)
    
grouped_n_ET %>%
    mutate(across(
        c('choseLL', 'choseLL_perc', 'x_count', 'OI', 'AI', 'PI'), 
        round, 1)
        )

subjects_biased_choice = grouped_n_ET %>%
    filter(n_choseLL<3) %>%
    arrange(run_id) %>%
    dplyr::pull(run_id) %>%
    unique()
print('Subjects with too many or too few choseLL: ')
print(subjects_biased_choice)

grouped_n_ET %>%
    filter(
        !(run_id %in% subjects_biased_choice) &
        !is.na(x_count) &
        (n_choseLL<3 | n_OI<3 | n_AI<3 | n_PI<3) |
        n_OI/n_choseLL<0.4 |
        n_AI/n_choseLL<0.4 |
        n_PI/n_choseLL<0.4)

subjects_NA_OI = grouped_n_ET %>%
    filter(n_OI<3 | n_OI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

subjects_NA_AI = grouped_n_ET %>%
    filter(n_AI<3 | n_AI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

subjects_NA_PI = grouped_n_ET %>%
    filter(n_PI<3 | n_PI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

print('Subjects without enough data for ET indices within one choice option: ')
print('OI')
subjects_NA_OI
print('AI')
subjects_NA_AI
print('PI')
subjects_NA_PI
```

## Visualize ET indices 
```{r}
data_plot = grouped_n_ET %>%
    filter(choseLL==1) %>%
    mutate(SE = sqrt(choseLL_perc * (1-choseLL_perc)/n_choseLL))

ggplot(data_plot, aes(factor(run_id), choseLL_perc)) + 
    geom_pointrange(aes(ymin=choseLL_perc-1.96*SE, 
                        ymax=choseLL_perc+1.96*SE)) +
    xlab('run_id') +
    ylab('Percent of choseLL') + 
    theme(
        axis.text.x = element_blank(),
        ) +
    coord_cartesian(ylim=c(0, 1))
```


# Cleaning
```{r}
# Data on subject level
excludeSubjects <- read.table('data_jupyter/excludeSubjects_choice.csv', 
                           header=TRUE, sep=',')[, 1]
excludeSubjects = c(
    excludeSubjects,
    subjects_biased_choice,
    subjects_NA_OI,
    subjects_NA_AI,
    subjects_NA_PI) %>% 
    sort() %>%
    unique()
excludeSubjects
print(paste('Total Number of subjects to exclude: ', length(excludeSubjects)))
```

```{r}
generalCleaninng = function(data) {
    data = data %>%
        filter(
            run_id < 1000 & 
            !(run_id %in% excludeSubjects)
                )
    return(data)
}

data_subject = generalCleaninng(data_subject)
data_trial = generalCleaninng(data_trial)

print(paste('Remaining of subjects: ', length(unique(data_trial$run_id))))
```
## data_subject
```{r}
print(paste('N subjects before cleaning: ', nrow(data_subject)))
data_subject = data_subject %>%
    filter(
        !is.na(attributeIndex) &
        !is.na(optionIndex) &
        !is.na(payneIndex)
    )
print(paste('N subjects after cleaning: ', nrow(data_subject)))
```
## data_trial
```{r}
print(paste('N subjects before cleaning: ', nrow(data_trial)))
data_trial = data_trial %>%
    filter(
        !is.na(attributeIndex) &
        !is.na(optionIndex) &
        !is.na(payneIndex)
    )
print(paste('N subjects after cleaning: ', nrow(data_trial)))
```

# Correlative Analysis

## Scatter matrix
````{r}
ggpairs(
    data_subject %>% dplyr::select(
        choseLL,
        attributeIndex, optionIndex, payneIndex, fps),     
    progress = F) 
ggsave("plots/choseLL_corr_01.pdf",width=6.5, height=5)

ggpairs(
    data_subject %>% dplyr::select(
        choseLL, birthyear, gender,
        LL_top, choice_rt, logK),
    progress = F) 
ggsave("plots/choseLL_corr_02.pdf",width=6.5, height=5)

ggpairs(
    data_trial %>% dplyr::select(
        choseLL,
        withinTaskIndex, trial_duration_exact,
        attributeIndex, optionIndex, payneIndex),     
    progress = F) 
ggsave("plots/choseLL_corr_01.pdf",width=6.5, height=5)

```

## Check withinTaskIndex and trial_duration_exact
```{r}
ggplot(data_trial, aes(x=factor(choseLL), y=withinTaskIndex)) + 
    geom_violin(trim=FALSE) +
    stat_summary(fun=mean, size=1)

ggplot(data_trial, aes(x=factor(choseLL), y=trial_duration_exact)) + 
    geom_violin(trim=FALSE) +
    stat_summary(fun=mean, size=1)
```

# Logistic Regression on subject-level
Sommet, 2017
 - choseLL is more likely than choseSS. 

 - Logisitic regression does not have residuals because it predicts probabilities not concrete values.

```{r}
data = data

m_null = glm(choseLL ~ 1,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m_null)
```

## Logisitic regression with predictors (m1)
A positive Attribute Index indicates more fixations on the amount attributes
A negative Option Index indicates more fixations on the delayed option
A negative Payne Index indicates more transitions within attributes 

 - Predict log-odds (exp(beta)). Odds increase by the factor exp(beta), when the predictor changes in one unit. beta=0 means odds=1. 
```{r results=FALSE, echo=FALSE, message=FALSE}
m1 = glm(choseLL ~ 1 + attributeIndex + optionIndex + payneIndex,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m1)
anova(m_null, m1)
```

# MLA
When I do not look at the summarized scores but on every trial, I need to account for the lack of interdependence and use a MLA.

## Intercept Only
https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
 - Odds for LL choice is exp(0.74) = 2.1 (Feb 02). Bias towards choseLL. The average chance for choseLL is exp(0.74)/(1+exp(0.74)) = 0.7. 
 - Once we control for the inter-subject variance, the choice shifts more towards SS (choseLL reduces from 65% to around 50%)
 - Mean log-odds for choice_LL vary across subjects. SD=0.4418, Odds could increase by factor exp(1.145)=3.14.
 - ICC=0.28, i.e. 18% of the variance are explained by between subject differences
````{r}
glmer0_io = glmer(
    choseLL ~ 1 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer0_io)

# take pi^2 / 3 instead of the level-1 residual
icc <- glmer0_io@theta[1]^2/ (glmer0_io@theta[1]^2 + (3.14159^2/3))
icc
```

## Within-Subject centering
```{r}
grouped = data.frame(
    data_subject$run_id,
    data_subject$attributeIndex,
    data_subject$optionIndex,
    data_subject$payneIndex
)
names(grouped) = c('run_id', 
                   'cluster_mean_AI', 
                   'cluster_mean_OI', 
                   'cluster_mean_PI')
for (var in c('cluster_mean_AI', 'cluster_mean_OI', 'cluster_mean_PI')) {
    if (var %in% names(data_trial)) {
        data_trial = data_trial %>% dplyr::select(!varName)
    }
}
data_trial = merge(
    data_trial,
    grouped, 
    by='run_id'
)
data_trial$attributeIndex_cmc = 
    data_trial$attributeIndex - 
    data_trial$cluster_mean_AI

data_trial$optionIndex_cmc = 
    data_trial$optionIndex - 
    data_trial$cluster_mean_OI

data_trial$payneIndex_cmc = 
    data_trial$payneIndex - 
    data_trial$cluster_mean_PI
data_trial
data_trial %>% 
    dplyr::select(attributeIndex, attributeIndex_cmc,
           optionIndex, optionIndex_cmc,
           payneIndex, payneIndex_cmc)
```


## Random Intercept (Constrained intermediate model)
Contains all theoretically relevant effects
```{r}
glmer1_ri = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer1_ri)
anova(glmer0_io, glmer1_ri)
```


## Random Intercept Random Slope (Augmented intermediate model)
Check if random slopes make sense.
```{r}
glmer2_rirs = glmer(
    choseLL ~  
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + 
          (attributeIndex_cmc + optionIndex_cmc + 
           payneIndex_cmc | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer2_rirs)
anova(glmer1_ri, glmer2_rirs)
```

## Fial model: RIRS
Control variables, such as age and withinTaskIndex do not show a correlation. 
Predictors: attributeIndex + optionIndex + payneIndex
 - Effect of the Option Index: Odds for choseLL increase by the factor exp(-0.97)=0.38. The more gaze points on immediate options, the lower the odd for choseLL
 - Effect of the Payne Index: Odds for choseLL increase by the factor exp(-0.17)=0.84. The more transitions within options (positive payne), the lower the odd for chose LL
 - Did not try to get the procentual reduction of L1 and L2 variance because the logistic regression does not have error terms

```{r}
glmer_final = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 1)
summary(glmer_final)

paste("FYI: The deviance of the CIM is:", glmer_final@devcomp$cmp[[8]])
# confint(m3_rirs, method="boot", n=50) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
```


## Simple Slopes
```{r}
# SD
sd_PI_cmc = sd(data_trial$payneIndex_cmc, 
               na.rm = TRUE)

# Simple Slope 1
data_trial$PI_sd1 = data_trial$payneIndex_cmc + sd_PI_cmc
glmer_final_sd1 = glmer(
    choseLL ~ attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc + 
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd1)

# Simple Slope -1
data_trial$PI_sd0 = data_trial$payneIndex_cmc - sd_PI_cmc
glmer_final_sd0 = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + PI_sd0 + 
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(glmer_final_sd0)
```

## Odds Ratios 
 - The effect of "payneIndex_cmc" is not significant, payneIndex_cmc OR = 0.83, 95% CI [0.67, 1] (has to be beyond 1 to be significant). Imagine it to be signficant: with an increase of the Payne Index of 1 (e.g. if they switch from neutral to only within option transitions), results in 0.67 times the chance (a lower chance) of choosing the delayed reward. 
 - Same for the ones with low (-1 SD) and high (+1 SD) Payne Index (If you have random slope, compare the different levels. 

```{R}
OR <- exp(fixef(glmer_final))
CI <- exp(confint(glmer_final, parm="beta_")) # it can be slow (~ a few minutes). As alternative, the much faster but less precise Wald's method can be used: CI <- exp(confint(FM,parm="beta_",method="Wald")) 

OR_SD1 <- exp(fixef(glmer_final_sd1))
CI_SD1 <- exp(confint(glmer_final_sd1, parm="beta_")) 

OR_SD0 <- exp(fixef(glmer_final_sd0))
CI_SD0 <- exp(confint(glmer_final_sd0, parm="beta_")) 

OR_CI<-rbind(
    cbind(OR,CI), 
    cbind(OR_SD1, CI_SD1)[3,], 
    cbind(OR_SD0, CI_SD0)[3,])

rownames(OR_CI) = c(
    rownames(cbind(OR,CI)), 
    "payne_cmc_sd1", 
    "payne_cmc_sd0")
OR_CI
```

MLA vs. simple logistic model
```{r}
GLMER = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc +
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
GLM = glm(
    choseLL ~ attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc, 
    data = data_trial, 
    family = binomial)

summary(GLMER)
summary(GLM)
anova(GLM, GLMER)
```


# Clusters
No clusters can improve the prediction of choices
```{r}

m_cluster_1 = glmer(
    choseLL ~ attributeIndex + optionIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10) 

m_cluster_2 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster2 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_3 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster3 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_4 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster4 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

fixef(m_cluster_2)[5]
fixef(m_cluster_3)[5]
fixef(m_cluster_4)[5]

# confint(m_cluster_2, method="boot", n=50) # CI with Bootstrap does not need normality
anova(m_cluster_1, m_cluster_2, m_cluster_3, m_cluster_4)
```

# Assumptions

## Linear relationship between predicted log(choseLL) and the predictors
Looks quite linear. If not check out: 
http://www.sthda.com/english/articles/36-classification-methods-essentials/148-logistic-regression-assumptions-and-diagnostics-in-r/
```{r}
predictors <- c(
    'attributeIndex_cmc', 'optionIndex_cmc', 
    'payneIndex_cmc')

data_plot = data_trial %>%
    mutate(predict_choseLL_prob = 
        predict(glmer_final, type = "response"),
        logit = log(predict_choseLL_prob/(1-predict_choseLL_prob))) %>%
    dplyr::select(c(logit, predictors)) %>%
    gather(key = "predictors", value = "predictor_value", -logit)
data_plot

ggplot(data_plot, aes(logit, predictor_value))+
    geom_point(size = 0.5, alpha = 0.5) +
    geom_smooth(method = "loess") + 
    theme_bw() + 
    facet_wrap(~predictors, scales = "free_y")
```

## Multicollinearity
compute variance inflation factor. " As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity"
```{r}
car::vif(glmer_final)
```