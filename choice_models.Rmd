---
title: "Choice Task Regression Analysis"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---


```{r}
setwd("C:/Users/User/GitHub/WebET_Analysis")
getPackages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

getPackages(c('colorspace', 
              'dplyr', 
              "effsize",
              'e1071',
              "ggplot2",
              "ggsignif",
              'lme4',
              'matlabr',
              'QuantPsyc',
              "RColorBrewer",
              'reshape2',
              'tidyr')
            )
```

# Matlab
```{r}
# run_matlab_script('amasino_dataPrep/fit_discount_k.m')
```

# Read and create datasets
## Exclude subjects
```{r}
# Data on subject level
excludeSubjects <- read.table('data_jupyter/excludeSubjects_choice.csv', 
                           header=TRUE, sep=',')[, 1]
excludeSubjects
```


## data_subject
```{r}
# Data on subject level
data_subject <- read.table('data_jupyter/data_subject.csv', 
                           header=TRUE, sep=',') %>%
    filter(!(run_id %in% excludeSubjects))
alllogk<-read.table(
    "amasino_dataPrep/intermediateCSVs/allLogk.csv", header=FALSE, sep=",")
names(alllogk) = c('run_id', 'logK')
data_subject = merge(data_subject, alllogk, by='run_id')
data_subject = data_subject %>% 
    filter(!is.na(logK))
print(nrow(data_subject))
data_subject
```


## data_trial_choice
```{r}
# Data on Trial level
data_trial_choice <- read.table('data_jupyter/data_trial_choice.csv',
                                header=TRUE, sep=',')  %>%
    filter(run_id %in% data_subject$run_id)

if ('logK' %in% names(data_trial_choice))
    {data_trial_choice = data_trial_choice %>% select(!logK)}
data_trial_choice = merge(
    data_trial_choice,
    data_subject %>% dplyr::select(run_id, logK),
    by='run_id'
)

data_trial_choice = data_trial_choice %>% filter(!is.na(logK))

print(paste('Number of subjects: ', length(unique(data_trial_choice$run_id))))
data_trial_choice %>% dplyr::select(run_id, trial_index, logK)
```

## data_et_choice
```{r}
data_et_choice <- read.table('data_jupyter/data_et_choice.csv', 
                             header=TRUE, sep=',') %>%
    filter(run_id %in% data_subject$run_id)

if ('choseTop' %in% names(data_et_choice))
    {data_et_choice = data_et_choice %>% select(!choseTop)}
data_et_choice = merge(
    data_et_choice, 
    data_trial_choice %>% 
        dplyr::group_by(run_id, trial_index) %>% 
        dplyr::summarise(choseTop = mean(choseTop)),
    by=c('run_id', 'trial_index')
)

data_et_choice$lookTopAOI = as.integer(data_et_choice$aoi == 'TL' | data_et_choice$aoi == 'TR')

print(paste('Number of subjects: ', length(unique(data_et_choice$run_id))))

```

# Screening
 - Response times > 4 or 5 seconds or malhalanobis? 

# Cleaning
see choice_analysis.Rmd



# Correlative Analysis
#````{r}
library(ggplot2) # Load the librarie (you have to do this one on each new session)
ggpairs(
    data_subject %>%
        dplyr::select(
            choseLL,
            attributeIndex, optionIndex, payneIndex,
            fps), 
    progress = F
    ) 
ggsave("plots/choseLL_corr_01.pdf",width=6.5, height=5)

ggpairs(
    data_subject %>%
        dplyr::select(
            choseLL,
            birthyear, gender,
            LL_top, choice_rt, logK),
    progress = F
    ) 
ggsave("plots/choseLL_corr_02.pdf",width=6.5, height=5)

#```

# Logistic Regression on subject-level
Sommet, 2017
 - Predict log-odds (exp(beta)). Odds increase by the factor exp(beta), when the predictor changes in one unit. beta=0 means odds=1. 
 - Logisitic regression does not have residuals because it predicts probabilities not concrete values.

```{r}
m0 = glm(choseLL ~ 1,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m0)
```

## M1: Adding predictors
A positive Attribute Index indicates more fixations on the amount attributes
A negative Option Index indicates more fixations on the delayed option
A negative Payne Index indicates more transitions within attributes 
```{r}
m1 = glm(choseLL ~ 1 + attributeIndex + optionIndex + payneIndex,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m1)
anova(m0, m1)
```

# MLA
When I do not look at the summarized scores but on every trial, I need to account for the lack of interdependence and use a MLA.

## Intercept Only
https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
 - Odds for LL choice is exp(-0.62) = 0.5379444 (Jan 28). The chance to choseLL is half as high as choseSS. The average chance is exp(-0.62)/(1+exp(-0.62)) = 0.3497815. 
 - Mean log-odds for choice_LL vary across subjects. SD=0.4418, Odds could increase by factor exp(0.44)=1.55.
 - ICC=0.056, i.e. 5.6% of the variance are explained by between subject differences
````{r}
m0_io = glmer(
    choseLL ~ 1 + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)
summary(m0_io)


# take pi^2 / 3 instead of the level-1 residual
icc <- m0_io@theta[1]^2/ (m0_io@theta[1]^2 + (3.14159^2/3))
icc
```
## Within-Subject centering
```{r}
grouped = data.frame(
    data_subject$run_id,
    data_subject$attributeIndex,
    data_subject$optionIndex,
    data_subject$payneIndex
)
names(grouped) = c('run_id', 
                   'cluster_mean_AI', 
                   'cluster_mean_OI', 
                   'cluster_mean_PI')
data_trial_choice = merge(
    data_trial_choice,
    grouped, 
    by='run_id'
)
data_trial_choice$attributeIndex_cmc = 
    data_trial_choice$attributeIndex - 
    data_trial_choice$cluster_mean_AI

data_trial_choice$optionIndex_cmc = 
    data_trial_choice$optionIndex - 
    data_trial_choice$cluster_mean_OI

data_trial_choice$payneIndex_cmc = 
    data_trial_choice$payneIndex - 
    data_trial_choice$cluster_mean_PI

data_trial_choice %>% 
    dplyr::select(attributeIndex, attributeIndex_cmc,
           optionIndex, optionIndex_cmc,
           payneIndex, payneIndex_cmc)
```


## Random Intercept (Constrained intermediate model)
Contains all theoretically relevant effects
```{r}
m1_ri = glmer(
    choseLL ~ withinTaskIndex + 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m1_ri)
paste("FYI: The deviance of the AIM is:", m1_ri@devcomp$cmp[[8]])
```


## Random Intercept Random Slope (Augmented intermediate model)
A likelihood ratio test (anova) shows that the model with random slopes is to be preferred.
```{r}
m2_rirs = glmer(
    choseLL ~ withinTaskIndex + 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + 
          (1 + attributeIndex_cmc + optionIndex_cmc + 
           payneIndex_cmc | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m2_rirs)
paste("FYI: The deviance of the CIM is:", m2_rirs@devcomp$cmp[[8]])
anova(m1_ri, m2_rirs)
```

## Fial model: RIRS
Control variables, such as age and withinTaskIndex do not show a correlation. 
Predictors: attributeIndex + optionIndex + payneIndex
 - Effect of the Option Index: Odds for choseLL increase by the factor exp(-0.97)=0.38. The more gaze points on immediate options, the lower the odd for choseLL
 - Effect of the Payne Index: Odds for choseLL increase by the factor exp(-0.17)=0.84. The more transitions within options (positive payne), the lower the odd for chose LL
 - Did not try to get the procentual reduction of L1 and L2 variance because the logistic regression does not have error terms

```{r}
m3_rirs = glmer(
    choseLL ~ withinTaskIndex + 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + 
          (1 + attributeIndex_cmc + optionIndex_cmc + 
           payneIndex_cmc | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m3_rirs)
paste("FYI: The deviance of the CIM is:", m3_rirs@devcomp$cmp[[8]])
confint(m3_rirs, method="boot", n=50) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
```

## Simple Slopes
```{r}
sd_OI_cmc <- sd(data_trial_choice$optionIndex_cmc)
sd_PI_cmc <- sd(data_trial_choice$payneIndex_cmc)

data_trial_choice$OI_sd1 =
    data_trial_choice$optionIndex_cmc + sd_OI_cmc

data_trial_choice$OI_sd1 =
    data_trial_choice$payneIndex_cmc + sd_PI_cmc

FM_m1SD <- glmer(bieber ~ gpa_m1SD + teacher_fan_c + (1 + gpa_cmc || classes) + gpa_m1SD:teacher_fan_c, data = d, family = "binomial")
summary(FM_m1SD)

#Below is the command to calculate the second simple slope
#namely the effect of "teacher_fan_c" when "gpa_cmc" is high (+1 SD).
d$gpa_p1SD <- d$gpa_cmc - sd_gpa_cmc
FM_p1SD <- glmer(bieber ~ gpa_p1SD + teacher_fan_c + (1 + gpa_cmc || classes) + gpa_p1SD:teacher_fan_c, data = d, family = "binomial")
summary(FM_p1SD)

#Below are the command to compare the coefficient estimates obtained in the final model,
#with (glmer) or without (glm) the use of multilevel modelling.
GLMER <- glmer(bieber ~ gpa_cmc + teacher_fan_c + (1 + gpa_cmc || classes) + gpa_cmc:teacher_fan_c, data = d, family = "binomial")
GLM <-  glm(bieber ~ gpa_cmc + teacher_fan_c + gpa_cmc:teacher_fan_c, data = d, family = "binomial")

summary(GLMER)
summary(GLM)
```


# Clusters
No clusters can improve the prediction of choices
```{r}

m_cluster_1 = glmer(
    choseLL ~ attributeIndex + optionIndex + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10) 

m_cluster_2 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster2 + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_3 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster3 + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_4 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster4 + (1 | run_id), 
    data = data_trial_choice, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

fixef(m_cluster_2)[5]
fixef(m_cluster_3)[5]
fixef(m_cluster_4)[5]

# confint(m_cluster_2, method="boot", n=50) # CI with Bootstrap does not need normality
anova(m_cluster_1, m_cluster_2, m_cluster_3, m_cluster_4)
```

## Visualize with Average Marginal Probability
#```{r}
summary(data_trial_choice$optionIndex)
```

#```{r}
tmpdat <- data_trial_choice[, c("choiceNr_c", 
                                "optionIndex",  
                                'attributeIndex', 
                                'payneIndex', 
                                'k',    
                                "run_id")]

# sequence of distinct values for predictor of interest
jvalues <- with(data_trial_choice, 
                seq(from = min(optionIndex), 
                    to = max(optionIndex), 
                    length.out = 100))

# calculate predicted probabilities and store in a list
predictions <- lapply(jvalues, function(j) {
    tmpdat$optionIndex <- j
    predict(model, newdata = tmpdat, type = "response")
})
```

#```{r}
# average marginal predicted probability across a few different 
plotdat <- sapply(predictions[c(1, 20, 40, 60, 80, 100)], mean, na.rm=TRUE)

# get the means with lower and upper quartiles
# plotdat <- t(sapply(predictions, function(x) {
#               c(M = mean(x), quantile(x,
#                                       c(0.25, 0.75),
#                                       na.rm=na_rm))
#     }
#   )
# )

# add in LengthofStay values and convert to data frame
plotdat <- as.data.frame(cbind(plotdat, jvalues))

# better names and show the first few rows

colnames(plotdat) <- c("PredictedProbability", "optionIndex")
ggplot(plotdat, aes(x = optionIndex, y = PredictedProbability)) + geom_line() +
    ylim(c(0, 1))
```

