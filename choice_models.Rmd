---
title: "Choice Task Regression Analysis"
author: "Tim Schneegans"
date: "28th of January 2021"
output: html_document
---


```{r}
setwd("C:/Users/User/GitHub/WebET_Analysis")
getPackages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

getPackages(c('colorspace', 
              'dplyr', 
              "effsize",
              'e1071',
              'GGally',
              "ggplot2",
              "ggsignif",
              'lme4',
              'matlabr',
              'QuantPsyc',
              "RColorBrewer",
              'reshape2',
              'tidyr')
            )
```

# Read and create datasets
```{r}
data_subject <- read.table('data_jupyter/data_subject.csv', 
                           header=TRUE, sep=',')
data_subject

data_trial <- read.table('data_jupyter/data_trial_choice.csv',
                                header=TRUE, sep=',')
data_trial
```

## k-values
K values and noise
```{r}
merge_by_subject = function(data, data_source, varName) {
    if (varName %in% names(data)) {
        data = data %>% dplyr::select(!varName)
    }

    data = merge(
        data, 
        data_source %>% dplyr::select(run_id, varName), 
        by='run_id')
    return(data)
}

# run_matlab_script('fitK/fit_discount_k.m')
logK <- read.table('fitK/logK.csv', header=TRUE, sep=',')
data_subject = merge_by_subject(data_subject, logK, 'logK')
data_subject = merge_by_subject(data_subject, logK, 'noise')

data_trial = merge_by_subject(data_trial, logK, 'noise')
data_trial = merge_by_subject(data_trial, logK, 'logK')
data_subject
data_subject %>%
    dplyr::select(run_id, logK, noise)
```

# Screening

## Criteria distribution. We need a sufficient ratio between the categories. Otherwise everything gets significant that happens in the larger category. 
```{r}
summary(factor(data_trial$choseLL))

summary(factor(data_trial$choseLL))[1] / 
    summary(factor(data_trial$choseLL))[2]
```

## Enough data points for the categorical predictors?
```{r}
table(data_trial$LL_top, data_trial$choseLL)
    
temp = data_trial %>% 
    merge_by_subject(data_subject, 'birthyear') %>% 
    merge_by_subject(data_subject, 'gender')

table(temp$birthyear, temp$choseLL)
table(temp$gender, temp$choseLL)
```

## Enough data points for the ET indices?
We need two more requirements to our data:
1) at least three trials with a valid option index
2) 40% of the trials have valid ET indices
```{r}
grouped_n_ET = data_trial %>% 
    group_by(run_id, choseLL) %>%
    summarise(
        n_choseLL = n(),
        choseLL_perc = n_choseLL/80,
        x_count = mean(x_count),
        n_OI = sum(!is.na(optionIndex)),
        OI = mean(optionIndex, na.rm=TRUE), 
        n_AI = sum(!is.na(attributeIndex)),
        AI = mean(attributeIndex, na.rm=TRUE),
        n_PI = sum(!is.na(payneIndex)),
        PI = mean(payneIndex, na.rm=TRUE)
        ) %>%
    arrange(run_id, choseLL)
    
grouped_n_ET %>%
    mutate(across(
        c('choseLL', 'choseLL_perc', 'x_count', 'OI', 'AI', 'PI'), 
        round, 1)
        )

subjects_biased_choice = grouped_n_ET %>%
    filter(n_choseLL<3) %>%
    arrange(run_id) %>%
    dplyr::pull(run_id) %>%
    unique()
print('Subjects with too many or too few choseLL: ')
print(subjects_biased_choice)

grouped_n_ET %>%
    filter(
        !(run_id %in% subjects_biased_choice) &
        !is.na(x_count) &
        (n_choseLL<3 | n_OI<3 | n_AI<3 | n_PI<3) |
        n_OI/n_choseLL<0.4 |
        n_AI/n_choseLL<0.4 |
        n_PI/n_choseLL<0.4)

subjects_NA_OI = grouped_n_ET %>%
    filter(n_OI<3 | n_OI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

subjects_NA_AI = grouped_n_ET %>%
    filter(n_AI<3 | n_AI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

subjects_NA_PI = grouped_n_ET %>%
    filter(n_PI<3 | n_PI/n_choseLL<0.4) %>%
    dplyr::pull(run_id) %>%
    unique()

print('Subjects without enough data for ET indices within one choice option: ')
print('OI')
subjects_NA_OI
print('AI')
subjects_NA_AI
print('PI')
subjects_NA_PI
```

## Visualize ET indices 
```{r}
data_plot = grouped_n_ET %>%
    filter(choseLL==1) %>%
    mutate(SE = sqrt(choseLL_perc * (1-choseLL_perc)/n_choseLL))

ggplot(data_plot, aes(factor(run_id), choseLL_perc)) + 
    geom_pointrange(aes(ymin=choseLL_perc-1.96*SE, 
                        ymax=choseLL_perc+1.96*SE)) +
    xlab('run_id') +
    ylab('Percent of choseLL') + 
    theme(
        axis.text.x = element_blank(),
        ) +
    coord_cartesian(ylim=c(0, 1))
```

# Cleaning
```{r}
# Data on subject level
excludeSubjects <- read.table('data_jupyter/excludeSubjects_choice.csv', 
                           header=TRUE, sep=',')[, 1]
excludeSubjects = c(
    excludeSubjects,
    subjects_biased_choice,
    subjects_NA_OI,
    subjects_NA_AI,
    subjects_NA_PI) %>% 
    sort() %>%
    unique()
excludeSubjects
print(paste('Total Number of subjects to exclude: ', length(excludeSubjects)))
```

```{r}
generalCleaninng = function(data) {
    data = data %>%
        filter(
            run_id < 1000 & 
            !(run_id %in% excludeSubjects)
                )
}

data_subject = generalCleaninng(data_subject)
data_trial = generalCleaninng(data_trial)
data_et = generalCleaninng(data_et)

print(paste('Remaining of subjects: ', length(unique(data_trial$run_id))))
```
## data_subject
```{r}
print(paste('N subjects before cleaning: ', nrow(data_subject)))
data_subject = data_subject %>%
    filter(
        !is.na(choseLL) & 
        !is.na(attributeIndex) &
        !is.na(optionIndex) &
        !is.na(payneIndex)
    )
print(paste('N subjects after cleaning: ', nrow(data_subject)))
```

# Correlative Analysis
````{r}
ggpairs(
    data_subject %>% dplyr::select(
        choseLL,
        attributeIndex, optionIndex, payneIndex, fps),     
    progress = F) 
ggsave("plots/choseLL_corr_01.pdf",width=6.5, height=5)

ggpairs(
    data_subject %>% dplyr::select(
        choseLL, birthyear, gender,
        LL_top, choice_rt, logK),
    progress = F) 
ggsave("plots/choseLL_corr_02.pdf",width=6.5, height=5)
```

# Logistic Regression on subject-level
Sommet, 2017
 - choseLL is more likely than choseSS. 

 - Logisitic regression does not have residuals because it predicts probabilities not concrete values.

```{r}
data = data

m_null = glm(choseLL ~ 1,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m_null)
```

## Logisitic regression with predictors (m1)
A positive Attribute Index indicates more fixations on the amount attributes
A negative Option Index indicates more fixations on the delayed option
A negative Payne Index indicates more transitions within attributes 

 - Predict log-odds (exp(beta)). Odds increase by the factor exp(beta), when the predictor changes in one unit. beta=0 means odds=1. 
```{r results=FALSE, echo=FALSE, message=FALSE}
m1 = glm(choseLL ~ 1 + attributeIndex + optionIndex + payneIndex,
         data = data_subject, 
         family = binomial(link = "logit"))
summary(m1)
anova(m_null, m1)
```

# MLA
When I do not look at the summarized scores but on every trial, I need to account for the lack of interdependence and use a MLA.

## Intercept Only
https://stats.idre.ucla.edu/r/dae/mixed-effects-logistic-regression/
 - Odds for LL choice is exp(-0.62) = 0.5379444 (Jan 28). The chance to choseLL is half as high as choseSS. The average chance is exp(-0.62)/(1+exp(-0.62)) = 0.3497815. 
 - Mean log-odds for choice_LL vary across subjects. SD=0.4418, Odds could increase by factor exp(0.44)=1.55.
 - ICC=0.36, i.e. 36% of the variance are explained by between subject differences
````{r}
m0_io = glmer(
    choseLL ~ 1 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)
summary(m0_io)


# take pi^2 / 3 instead of the level-1 residual
icc <- m0_io@theta[1]^2/ (m0_io@theta[1]^2 + (3.14159^2/3))
icc
```

## Within-Subject centering
```{r}
grouped = data.frame(
    data_subject$run_id,
    data_subject$attributeIndex,
    data_subject$optionIndex,
    data_subject$payneIndex
)
names(grouped) = c('run_id', 
                   'cluster_mean_AI', 
                   'cluster_mean_OI', 
                   'cluster_mean_PI')
data_trial = merge(
    data_trial,
    grouped, 
    by='run_id'
)
data_trial$attributeIndex_cmc = 
    data_trial$attributeIndex - 
    data_trial$cluster_mean_AI

data_trial$optionIndex_cmc = 
    data_trial$optionIndex - 
    data_trial$cluster_mean_OI

data_trial$payneIndex_cmc = 
    data_trial$payneIndex - 
    data_trial$cluster_mean_PI

data_trial %>% 
    dplyr::select(attributeIndex, attributeIndex_cmc,
           optionIndex, optionIndex_cmc,
           payneIndex, payneIndex_cmc)
```


## Random Intercept (Constrained intermediate model)
Contains all theoretically relevant effects
```{r}
m1_ri = glmer(
    choseLL ~ withinTaskIndex + 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m1_ri)
```


## Random Intercept Random Slope (Augmented intermediate model)
A likelihood ratio test (anova) shows that the model with smaller deviance and smaller AIC and BIC criteria is to be preferred.
```{r}
m2_rirs = glmer(
    choseLL ~ withinTaskIndex + 
        attributeIndex_cmc + optionIndex_cmc + 
        payneIndex_cmc + 
          (1 + attributeIndex_cmc + optionIndex_cmc + 
           payneIndex_cmc | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m2_rirs)
anova(m1_ri, m2_rirs)
```

## Fial model: RIRS
Control variables, such as age and withinTaskIndex do not show a correlation. 
Predictors: attributeIndex + optionIndex + payneIndex
 - Effect of the Option Index: Odds for choseLL increase by the factor exp(-0.97)=0.38. The more gaze points on immediate options, the lower the odd for choseLL
 - Effect of the Payne Index: Odds for choseLL increase by the factor exp(-0.17)=0.84. The more transitions within options (positive payne), the lower the odd for chose LL
 - Did not try to get the procentual reduction of L1 and L2 variance because the logistic regression does not have error terms

```{r}
m3_final = glmer(
    choseLL ~ attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc + 
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m3_rirs)

paste("FYI: The deviance of the CIM is:", m3_rirs@devcomp$cmp[[8]])
# confint(m3_rirs, method="boot", n=50) # CI with Bootstrap
# The confidence intervals should not include 1 to be significant
```


## Simple Slopes
```{r}
# SD
sd_PI_cmc = sd(data_trial$payneIndex_cmc, 
               na.rm = TRUE)

# Simple Slope 1
data_trial$PI_sd1 = data_trial$payneIndex_cmc + sd_PI_cmc
m3_sd1 = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc + 
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m3_sd1)

# Simple Slope -1
data_trial$PI_sd0 = data_trial$payneIndex_cmc - sd_PI_cmc
m3_sd0 = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + PI_sd0 + 
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
summary(m3_sd0)
```

## Odds Ratios 
 - The effect of "payneIndex_cmc" is not significant, payneIndex_cmc OR = 0.83, 95% CI [0.67, 1] (has to be beyond 1 to be significant). Imagine it to be signficant: with an increase of the Payne Index of 1 (e.g. if they switch from neutral to only within option transitions), results in 0.67 times the chance (a lower chance) of choosing the delayed reward. 
 - Same for the ones with low (-1 SD) and high (+1 SD) Payne Index (If you have random slope, compare the different levels. 

```{R}
OR <- exp(fixef(m3_final))
CI <- exp(confint(m3_final, parm="beta_")) # it can be slow (~ a few minutes). As alternative, the much faster but less precise Wald's method can be used: CI <- exp(confint(FM,parm="beta_",method="Wald")) 

OR_SD1 <- exp(fixef(m3_sd1))
CI_SD1 <- exp(confint(m3_sd1, parm="beta_")) 

OR_SD0 <- exp(fixef(m3_sd0))
CI_SD0 <- exp(confint(m3_sd0, parm="beta_")) 

OR_CI<-rbind(
    cbind(OR,CI), 
    cbind(OR_SD1, CI_SD1)[3,], 
    cbind(OR_SD0, CI_SD0)[3,])

rownames(OR_CI) = c(
    rownames(cbind(OR,CI)), 
    "payne_cmc_sd1", 
    "payne_cmc_sd0")
OR_CI
```

MLA vs. simple logistic model
```{r}
GLMER = glmer(
    choseLL ~ 
        attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc +
        (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"))
GLM = glm(
    choseLL ~ attributeIndex_cmc + optionIndex_cmc + payneIndex_cmc, 
    data = data_trial, 
    family = binomial)

summary(GLMER)
summary(GLM)
anova(GLM, GLMER)
```


# Clusters
No clusters can improve the prediction of choices
```{r}

m_cluster_1 = glmer(
    choseLL ~ attributeIndex + optionIndex + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10) 

m_cluster_2 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster2 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_3 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster3 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

m_cluster_4 = glmer(
    choseLL ~ attributeIndex + optionIndex + 
        cluster4 + (1 | run_id), 
    data = data_trial, 
    family = binomial, 
    control = glmerControl(optimizer = "bobyqa"),
    nAGQ = 10)

fixef(m_cluster_2)[5]
fixef(m_cluster_3)[5]
fixef(m_cluster_4)[5]

# confint(m_cluster_2, method="boot", n=50) # CI with Bootstrap does not need normality
anova(m_cluster_1, m_cluster_2, m_cluster_3, m_cluster_4)
```

## Visualize with Average Marginal Probability
#```{r}
summary(data_trial$optionIndex)
```

#```{r}
tmpdat <- data_trial[, c("choiceNr_c", 
                                "optionIndex",  
                                'attributeIndex', 
                                'payneIndex', 
                                'k',    
                                "run_id")]

# sequence of distinct values for predictor of interest
jvalues <- with(data_trial, 
                seq(from = min(optionIndex), 
                    to = max(optionIndex), 
                    length.out = 100))

# calculate predicted probabilities and store in a list
predictions <- lapply(jvalues, function(j) {
    tmpdat$optionIndex <- j
    predict(model, newdata = tmpdat, type = "response")
})
```

#```{r}
# average marginal predicted probability across a few different 
plotdat <- sapply(predictions[c(1, 20, 40, 60, 80, 100)], mean, na.rm=TRUE)

# get the means with lower and upper quartiles
# plotdat <- t(sapply(predictions, function(x) {
#               c(M = mean(x), quantile(x,
#                                       c(0.25, 0.75),
#                                       na.rm=na_rm))
#     }
#   )
# )

# add in LengthofStay values and convert to data frame
plotdat <- as.data.frame(cbind(plotdat, jvalues))

# better names and show the first few rows

colnames(plotdat) <- c("PredictedProbability", "optionIndex")
ggplot(plotdat, aes(x = optionIndex, y = PredictedProbability)) + geom_line() +
    ylim(c(0, 1))
```

